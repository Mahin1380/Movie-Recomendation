{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4fff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK resources are available only once\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Set up stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7853e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by lowercasing, removing non-alphabetic characters,\n",
    "    removing stopwords, and applying lemmatization.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Path setup\n",
    "base_dir = os.getcwd()\n",
    "file_path = os.path.join(base_dir, \"movies.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "# Remove unwanted columns (if exists)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_pickle(csv_path, output_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Clean text columns (apply the clean_text function)\n",
    "    df['cleaned_overview'] = df['overview'].apply(clean_text)\n",
    "    df['genres'] = df['genres'].apply(clean_text)\n",
    "    \n",
    "    # Create a 'tags' column by combining 'genres' and 'cleaned_overview'\n",
    "    df[\"tags\"] = df[\"genres\"] + ' ' + df[\"cleaned_overview\"]\n",
    "    df['tags'] = df['tags'].fillna('')  # Fill any NaN values with an empty string\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    # Initialize the TF-IDF vectorizer with desired parameters\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_df=0.8,        # Ignore overly common terms\n",
    "        min_df=5,          # Ignore rare terms (in fewer than 5 docs)\n",
    "        ngram_range=(1, 2),# Use unigrams + bigrams\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the tags into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(df['tags'])\n",
    "    \n",
    "    # Compute cosine similarity between movies using the TF-IDF matrix\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Save the TF-IDF matrix and cosine similarity matrix in the pickle file\n",
    "    try:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump((tfidf_matrix, cosine_sim), f)\n",
    "        print(f\"Pickle file saved successfully as {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349ef1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file saved successfully as tfidf_model3.pkl\n",
      "Pickle file loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and pickle the model\n",
    "train_and_pickle(file_path)\n",
    "\n",
    "# Later when you load your pickled model, simply do:\n",
    "try:\n",
    "    with open('tfidf_model3.pkl', 'rb') as f:\n",
    "        tfidf_matrix, cosine_sim = pickle.load(f)\n",
    "    print(\"Pickle file loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0974c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Wolf of Wall Street', \"Mothers' Instinct\", 'Carry-On', 'The Collection', 'Mortal Kombat']\n"
     ]
    }
   ],
   "source": [
    "def load_pickled_model():\n",
    "    # Load the dataframe from CSV\n",
    "    df = pd.read_csv(os.path.join(base_dir, \"movies.csv\"))\n",
    "    \n",
    "    # Unpack only the two objects (tfidf_matrix and cosine_sim) from the pickle file\n",
    "    tfidf_matrix, cosine_sim = pickle.load(\n",
    "        open(os.path.join(base_dir, \"tfidf_model3.pkl\"), \"rb\")\n",
    "    )\n",
    "    \n",
    "    return tfidf_matrix, cosine_sim, df\n",
    "\n",
    "def content_based_recommender_with_pickled_model(\n",
    "    movie_title, top_n=5, tfidf=None, cosine_sim=None, df=None\n",
    "):\n",
    "    # Create a mapping of movie titles to index\n",
    "    indices = pd.Series(df.index, index=df[\"title\"]).drop_duplicates()\n",
    "\n",
    "    # Get the index of the input movie title\n",
    "    idx = indices.get(movie_title, None)\n",
    "    if idx is None:\n",
    "        return f\"Movie '{movie_title}' not found in dataset.\"\n",
    "\n",
    "    # Get similarity scores and sort them\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get all similar movies (excluding the input movie itself)\n",
    "    sim_indices = [i[0] for i in sim_scores[1:21]]  # Get top 20 for replacement pool\n",
    "    \n",
    "    # Ensure we return only the top_n recommendations\n",
    "    sim_indices = sim_indices[:top_n]  # Slice to return only top_n recommendations\n",
    "    \n",
    "    # Return recommended movies\n",
    "    return df[\"title\"].iloc[sim_indices].tolist()\n",
    "\n",
    "# Load the pickled model and DataFrame\n",
    "tfidf_matrix, cosine_sim, df = load_pickled_model()\n",
    "\n",
    "# Example usage: get top 5 recommended movies for \"Dune\"\n",
    "recommended_movies = content_based_recommender_with_pickled_model(movie_title=\"Dune: Part Two\", top_n=5, tfidf=tfidf_matrix, cosine_sim=cosine_sim, df=df)\n",
    "print(recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b935af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
